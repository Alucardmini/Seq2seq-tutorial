{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实现双向的dynamic_lstm+beam_search\n",
    "### 基于tensorflow1.4 Seq2seq的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder使用的是双向的LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "产生10个长度不一（最短3，最长8）的sequences, 其中前十个是:\n",
      "[6, 5, 7, 2]\n",
      "[3, 7, 2, 9, 7, 8]\n",
      "[2, 9, 2, 8, 9]\n",
      "[7, 7, 8]\n",
      "[6, 5, 6, 7, 9, 2, 7]\n",
      "[6, 2, 3, 6]\n",
      "[4, 7, 7]\n",
      "[6, 7, 7, 4, 8, 3, 2, 3]\n",
      "[8, 4, 5, 4]\n",
      "[3, 3, 5, 9, 4]\n"
     ]
    }
   ],
   "source": [
    "import helpers\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.util import nest\n",
    "from tensorflow.contrib import seq2seq,rnn\n",
    "\n",
    "tf.__version__\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "PAD = 0\n",
    "EOS = 1\n",
    "\n",
    "\n",
    "vocab_size = 10\n",
    "input_embedding_size = 20\n",
    "encoder_hidden_units = 25\n",
    "\n",
    "decoder_hidden_units = encoder_hidden_units\n",
    "\n",
    "import helpers as data_helpers\n",
    "batch_size = 10\n",
    "\n",
    "# 一个generator，每次产生一个minibatch的随机样本\n",
    "\n",
    "batches = data_helpers.random_sequences(length_from=3, length_to=8,\n",
    "                                   vocab_lower=2, vocab_upper=10,\n",
    "                                   batch_size=batch_size)\n",
    "\n",
    "print('产生%d个长度不一（最短3，最长8）的sequences, 其中前十个是:' % batch_size)\n",
    "for seq in next(batches)[:min(batch_size, 10)]:\n",
    "    print(seq)\n",
    "    \n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "mode = tf.contrib.learn.ModeKeys.TRAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.使用seq2seq库实现seq2seq模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 计算图的数据的placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('minibatch'):\n",
    "    encoder_inputs = tf.placeholder(tf.int32, [None, None], name='encoder_inputs')\n",
    "    \n",
    "    encoder_inputs_length = tf.placeholder(tf.int32, [None], name='encoder_inputs_length')\n",
    "    \n",
    "    decoder_targets = tf.placeholder(tf.int32, [None, None], name='decoder_targets')\n",
    "    \n",
    "    decoder_inputs = tf.placeholder(shape=(None, None),dtype=tf.int32,name='decoder_inputs')\n",
    "    \n",
    "    #decoder_inputs_length和decoder_targets_length是一样的\n",
    "    decoder_inputs_length = tf.placeholder(shape=(None,),\n",
    "                                            dtype=tf.int32,\n",
    "                                            name='decoder_inputs_length')\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.设置embedding部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 构建embedding矩阵,encoder和decoder公用该词向量矩阵\n",
    "embedding = tf.get_variable('embedding', [vocab_size,input_embedding_size])\n",
    "encoder_inputs_embedded = tf.nn.embedding_lookup(embedding,encoder_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.定义lstm_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fw_cell = bw_cell =  rnn.LSTMCell(encoder_hidden_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.定义encoder 部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('encoder'):\n",
    "    ((encoder_fw_outputs,\n",
    "  encoder_bw_outputs),\n",
    " (encoder_fw_final_state,\n",
    "  encoder_bw_final_state)) = (\n",
    "    tf.nn.bidirectional_dynamic_rnn(cell_fw=fw_cell,\n",
    "                                    cell_bw=bw_cell,\n",
    "                                    inputs=encoder_inputs_embedded,\n",
    "                                    sequence_length=encoder_inputs_length,\n",
    "                                    dtype=tf.float32, time_major=False)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'encoder/bidirectional_rnn/fw/fw/transpose:0' shape=(?, ?, 25) dtype=float32>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    encoder_fw_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'encoder/ReverseSequence:0' shape=(?, ?, 25) dtype=float32>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    encoder_bw_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMStateTuple(c=<tf.Tensor 'encoder/bidirectional_rnn/fw/fw/while/Exit_2:0' shape=(?, 25) dtype=float32>, h=<tf.Tensor 'encoder/bidirectional_rnn/fw/fw/while/Exit_3:0' shape=(?, 25) dtype=float32>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    encoder_fw_final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMStateTuple(c=<tf.Tensor 'encoder/bidirectional_rnn/bw/bw/while/Exit_2:0' shape=(?, 25) dtype=float32>, h=<tf.Tensor 'encoder/bidirectional_rnn/bw/bw/while/Exit_3:0' shape=(?, 25) dtype=float32>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    encoder_bw_final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对encoder的输出进行合并\n",
    "输出： \n",
    "outputs是一个(output_fw, output_bw)元组，output_fw和output_bw的shape都是[batch_size, sequence_length, num_units]\n",
    "\n",
    "output_states是一个(output_state_fw, output_state_bw) 元组，分别是前向和后向最后一个Cell的Output，output_state_fw和output_state_bw的类型都是LSTMStateTuple，这个类有两个属性c和h，分别表示Memory Cell和Hidden State，如下图： \n",
    "![alt text](figure/basic_lstm_cell.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs), 2)\n",
    "\n",
    "    encoder_final_state_h = tf.concat((encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n",
    "    encoder_final_state_c = tf.concat((encoder_fw_final_state.c, encoder_bw_final_state.c), 1)\n",
    "    encoder_final_state = rnn.LSTMStateTuple(\n",
    "        c=encoder_final_state_c,\n",
    "        h=encoder_final_state_h\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMStateTuple(c=<tf.Tensor 'concat_2:0' shape=(?, 50) dtype=float32>, h=<tf.Tensor 'concat_1:0' shape=(?, 50) dtype=float32>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    encoder_final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.定义decoder 部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t<tf.Tensor 'decoder/Identity:0' shape=(?, ?, 10) dtype=float32>\n",
      "\t<tf.Tensor 'minibatch/decoder_targets:0' shape=(?, ?) dtype=int32>\n",
      "\t<tf.Tensor 'decoder/decoder/transpose_1:0' shape=(?, ?) dtype=int32>\n"
     ]
    }
   ],
   "source": [
    "def _create_rnn_cell2():\n",
    "    def single_rnn_cell(encoder_hidden_units):\n",
    "        # 创建单个cell，这里需要注意的是一定要使用一个single_rnn_cell的函数，不然直接把cell放在MultiRNNCell\n",
    "        # 的列表中最终模型会发生错误\n",
    "        single_cell = rnn.LSTMCell(encoder_hidden_units*2)\n",
    "        #添加dropout\n",
    "        single_cell = rnn.DropoutWrapper(single_cell, output_keep_prob=0.5)\n",
    "        return single_cell\n",
    "            #列表中每个元素都是调用single_rnn_cell函数\n",
    "            #cell = rnn.MultiRNNCell([single_rnn_cell() for _ in range(self.num_layers)])\n",
    "    cell = rnn.MultiRNNCell([single_rnn_cell(encoder_hidden_units) for _ in range(1)])\n",
    "    return cell \n",
    "\n",
    "with tf.variable_scope('decoder'):\n",
    "    #single_cell = rnn.LSTMCell(encoder_hidden_units)\n",
    "    #decoder_cell = rnn.MultiRNNCell([single_cell for _ in range(1)])\n",
    "    decoder_cell = rnn.LSTMCell(encoder_hidden_units*2)\n",
    "    #定义decoder的初始状态\n",
    "    decoder_initial_state = encoder_final_state\n",
    "    \n",
    "    #定义output_layer\n",
    "    output_layer = tf.layers.Dense(vocab_size,kernel_initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.1))\n",
    "    \n",
    "    decoder_inputs_embedded = tf.nn.embedding_lookup(embedding, decoder_inputs)\n",
    "    \n",
    "    # 训练阶段，使用TrainingHelper+BasicDecoder的组合，这一般是固定的，当然也可以自己定义Helper类，实现自己的功能\n",
    "    training_helper = seq2seq.TrainingHelper(inputs=decoder_inputs_embedded,\n",
    "                                                        sequence_length=decoder_inputs_length,\n",
    "                                                        time_major=False, name='training_helper')\n",
    "    training_decoder = seq2seq.BasicDecoder(cell=decoder_cell, helper=training_helper,\n",
    "                                                       initial_state=decoder_initial_state,\n",
    "                                                       output_layer=output_layer)\n",
    "    \n",
    "    # 调用dynamic_decode进行解码，decoder_outputs是一个namedtuple，里面包含两项(rnn_outputs, sample_id)\n",
    "    # rnn_output: [batch_size, decoder_targets_length, vocab_size]，保存decode每个时刻每个单词的概率，可以用来计算loss\n",
    "    # sample_id: [batch_size], tf.int32，保存最终的编码结果。可以表示最后的答案\n",
    "    max_target_sequence_length = tf.reduce_max(decoder_inputs_length, name='max_target_len')\n",
    "    decoder_outputs, _, _ = seq2seq.dynamic_decode(decoder=training_decoder,\n",
    "                                                          impute_finished=True,\n",
    "                                                          maximum_iterations=max_target_sequence_length)\n",
    "    decoder_logits_train = tf.identity(decoder_outputs.rnn_output)\n",
    "    sample_id = decoder_outputs.sample_id\n",
    "    max_target_sequence_length = tf.reduce_max(decoder_inputs_length, name='max_target_len')\n",
    "    mask = tf.sequence_mask(decoder_inputs_length,max_target_sequence_length, dtype=tf.float32, name='masks')\n",
    "    print('\\t%s' % repr(decoder_logits_train))\n",
    "    print('\\t%s' % repr(decoder_targets))\n",
    "    print('\\t%s' % repr(sample_id))\n",
    "    loss = seq2seq.sequence_loss(logits=decoder_logits_train,targets=decoder_targets, weights=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('decoder',reuse=True):\n",
    "    start_tokens = tf.ones([batch_size, ], tf.int32)*1  #[batch_size]  数值为1\n",
    "    encoder_state = nest.map_structure(lambda s: seq2seq.tile_batch(s, 3),\n",
    "                                                   encoder_final_state)\n",
    "    inference_decoder = tf.contrib.seq2seq.BeamSearchDecoder(cell=decoder_cell, embedding=embedding,\n",
    "                                                                             start_tokens=start_tokens,\n",
    "                                                                             end_token=1,\n",
    "                                                                             initial_state=encoder_state,\n",
    "                                                                             beam_width=3,\n",
    "                                                                             output_layer=output_layer)\n",
    "    beam_decoder_outputs, _, _ = seq2seq.dynamic_decode(decoder=inference_decoder,maximum_iterations=10)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_inputs:\n",
      "[6 6 3 4 9 7 4 7]\n",
      "encoder_inputs_length:\n",
      "8\n",
      "decoder_inputs:\n",
      "[1 6 6 3 4 9 7 4 7]\n",
      "decoder_inputs_length:\n",
      "9\n",
      "decoder_targets:\n",
      "[6 6 3 4 9 7 4 7 1]\n"
     ]
    }
   ],
   "source": [
    "train_op = tf.train.AdamOptimizer(learning_rate = 0.001).minimize(loss)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "def next_feed():\n",
    "    batch = next(batches)\n",
    "    \n",
    "    encoder_inputs_, encoder_inputs_length_ = data_helpers.batch(batch)\n",
    "    decoder_targets_, decoder_targets_length_ = data_helpers.batch(\n",
    "        [(sequence) + [EOS] for sequence in batch]\n",
    "    )\n",
    "    decoder_inputs_, decoder_inputs_length_ = data_helpers.batch(\n",
    "        [[EOS] + (sequence) for sequence in batch]\n",
    "    )\n",
    "    \n",
    "    # 在feedDict里面，key可以是一个Tensor\n",
    "    return {\n",
    "        encoder_inputs: encoder_inputs_.T,\n",
    "        decoder_inputs: decoder_inputs_.T,\n",
    "        decoder_targets: decoder_targets_.T,\n",
    "        encoder_inputs_length: encoder_inputs_length_,\n",
    "        decoder_inputs_length: decoder_inputs_length_\n",
    "    }\n",
    "\n",
    "x = next_feed()\n",
    "print('encoder_inputs:')\n",
    "print(x[encoder_inputs][0,:])\n",
    "print('encoder_inputs_length:')\n",
    "print(x[encoder_inputs_length][0])\n",
    "print('decoder_inputs:')\n",
    "print(x[decoder_inputs][0,:])\n",
    "print('decoder_inputs_length:')\n",
    "print(x[decoder_inputs_length][0])\n",
    "print('decoder_targets:')\n",
    "print(x[decoder_targets][0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "  minibatch loss: 2.3011417388916016\n",
      "  sample 1:\n",
      "    input     > [4 3 6 6 3 7 7 5]\n",
      "    predicted > [[9 9 9]\n",
      " [9 9 9]\n",
      " [6 6 6]\n",
      " [6 6 6]\n",
      " [6 6 6]\n",
      " [5 6 6]\n",
      " [6 5 5]\n",
      " [6 6 6]\n",
      " [6 6 6]\n",
      " [5 5 8]]\n",
      "  sample 2:\n",
      "    input     > [3 5 3 4 7 0 0 0]\n",
      "    predicted > [[ 0  0  0]\n",
      " [ 4  4  4]\n",
      " [ 4  4  4]\n",
      " [ 4  4  4]\n",
      " [ 9  9  9]\n",
      " [ 4  4  4]\n",
      " [ 9  9  9]\n",
      " [ 7  7  7]\n",
      " [ 1  7  1]\n",
      " [ 1  7 -1]]\n",
      "  sample 3:\n",
      "    input     > [7 4 8 9 9 2 0 0]\n",
      "    predicted > [[ 9  9  9]\n",
      " [ 9  9  9]\n",
      " [ 5  5  5]\n",
      " [ 0  0  0]\n",
      " [ 7  6  6]\n",
      " [ 1  6  6]\n",
      " [-1  7  7]\n",
      " [-1  7  7]\n",
      " [-1  7  7]\n",
      " [-1  2  7]]\n",
      "\n",
      "batch 200\n",
      "  minibatch loss: 1.5333470106124878\n",
      "  sample 1:\n",
      "    input     > [6 7 4 2 7 3 0]\n",
      "    predicted > [[6 6 6]\n",
      " [6 6 6]\n",
      " [7 7 3]\n",
      " [3 3 7]\n",
      " [7 3 3]\n",
      " [3 3 3]\n",
      " [1 1 1]]\n",
      "  sample 2:\n",
      "    input     > [7 5 8 5 0 0 0]\n",
      "    predicted > [[ 5  5  7]\n",
      " [ 5  5  5]\n",
      " [ 5  5  5]\n",
      " [ 1  1  5]\n",
      " [ 1 -1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 3:\n",
      "    input     > [7 9 7 6 7 2 0]\n",
      "    predicted > [[5 5 5]\n",
      " [7 7 7]\n",
      " [7 3 7]\n",
      " [3 7 3]\n",
      " [7 7 3]\n",
      " [3 3 3]\n",
      " [1 1 1]]\n",
      "\n",
      "batch 400\n",
      "  minibatch loss: 1.137063980102539\n",
      "  sample 1:\n",
      "    input     > [2 5 4 8 9 5 2 8]\n",
      "    predicted > [[9 9 9]\n",
      " [5 5 5]\n",
      " [8 8 8]\n",
      " [4 4 4]\n",
      " [5 5 5]\n",
      " [4 4 4]\n",
      " [4 5 5]\n",
      " [5 8 4]\n",
      " [1 1 1]]\n",
      "  sample 2:\n",
      "    input     > [3 9 3 9 0 0 0 0]\n",
      "    predicted > [[ 3  9  3]\n",
      " [ 9  3  9]\n",
      " [ 9  3  3]\n",
      " [ 3  9  9]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 3:\n",
      "    input     > [9 4 2 8 5 0 0 0]\n",
      "    predicted > [[ 9  9  9]\n",
      " [ 4  4  4]\n",
      " [ 5  5  5]\n",
      " [ 4  4  8]\n",
      " [ 4  8  4]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "\n",
      "batch 600\n",
      "  minibatch loss: 0.8631468415260315\n",
      "  sample 1:\n",
      "    input     > [9 5 2 9 6 8 6 9]\n",
      "    predicted > [[5 5 5]\n",
      " [9 9 9]\n",
      " [9 9 9]\n",
      " [6 6 6]\n",
      " [9 9 2]\n",
      " [4 6 9]\n",
      " [6 4 6]\n",
      " [7 5 8]\n",
      " [1 1 1]]\n",
      "  sample 2:\n",
      "    input     > [5 7 9 0 0 0 0 0]\n",
      "    predicted > [[ 5  7  9]\n",
      " [ 7  9  7]\n",
      " [ 9  5  5]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 3:\n",
      "    input     > [8 2 9 0 0 0 0 0]\n",
      "    predicted > [[ 8  4  4]\n",
      " [ 9  9  9]\n",
      " [ 2  7  2]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "\n",
      "batch 800\n",
      "  minibatch loss: 0.7218129634857178\n",
      "  sample 1:\n",
      "    input     > [3 4 6 7 2 6 0 0]\n",
      "    predicted > [[ 6  6  6]\n",
      " [ 2  3  3]\n",
      " [ 3  2  2]\n",
      " [ 2  2  2]\n",
      " [ 6  6  6]\n",
      " [ 7  5  2]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 2:\n",
      "    input     > [5 5 2 8 0 0 0 0]\n",
      "    predicted > [[ 5  5  5]\n",
      " [ 5  5  5]\n",
      " [ 2  5  5]\n",
      " [ 8  6  8]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 3:\n",
      "    input     > [4 3 9 0 0 0 0 0]\n",
      "    predicted > [[ 4  8  9]\n",
      " [ 3  9  6]\n",
      " [ 9  3  4]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "\n",
      "batch 1000\n",
      "  minibatch loss: 0.42369818687438965\n",
      "  sample 1:\n",
      "    input     > [8 8 4 0 0 0 0]\n",
      "    predicted > [[ 8  8  4]\n",
      " [ 8  4  8]\n",
      " [ 4  8  8]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 2:\n",
      "    input     > [6 3 5 4 0 0 0]\n",
      "    predicted > [[ 6  6  3]\n",
      " [ 3  5  4]\n",
      " [ 5  3  6]\n",
      " [ 4  4  5]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 3:\n",
      "    input     > [9 5 5 0 0 0 0]\n",
      "    predicted > [[ 5  9  5]\n",
      " [ 9  5  9]\n",
      " [ 5  5  9]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "\n",
      "batch 1200\n",
      "  minibatch loss: 0.43877652287483215\n",
      "  sample 1:\n",
      "    input     > [4 6 8 9 2 0 0 0]\n",
      "    predicted > [[ 4  8  8]\n",
      " [ 6  4  6]\n",
      " [ 8  6  4]\n",
      " [ 9  2  9]\n",
      " [ 2  9  2]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 2:\n",
      "    input     > [8 9 2 0 0 0 0 0]\n",
      "    predicted > [[ 8  9  4]\n",
      " [ 9  8  9]\n",
      " [ 2  4  3]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 3:\n",
      "    input     > [4 4 5 5 4 7 7 0]\n",
      "    predicted > [[ 4  4  4]\n",
      " [ 4  4  4]\n",
      " [ 5  5  5]\n",
      " [ 5  4  7]\n",
      " [ 4  7  4]\n",
      " [ 7  5  5]\n",
      " [ 7  7  5]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]]\n",
      "\n",
      "batch 1400\n",
      "  minibatch loss: 0.37541431188583374\n",
      "  sample 1:\n",
      "    input     > [3 9 7 5 8 6 0]\n",
      "    predicted > [[ 3  3  3]\n",
      " [ 9  5  5]\n",
      " [ 5  3  9]\n",
      " [ 7  9  3]\n",
      " [ 6  8  6]\n",
      " [ 8  7  5]\n",
      " [ 1  1  8]\n",
      " [-1 -1  1]]\n",
      "  sample 2:\n",
      "    input     > [4 6 9 7 4 9 6]\n",
      "    predicted > [[4 4 4]\n",
      " [6 6 4]\n",
      " [9 9 3]\n",
      " [4 4 6]\n",
      " [7 7 9]\n",
      " [9 3 7]\n",
      " [6 4 4]\n",
      " [1 1 1]]\n",
      "  sample 3:\n",
      "    input     > [8 8 2 2 7 8 9]\n",
      "    predicted > [[8 8 8]\n",
      " [2 8 2]\n",
      " [8 2 8]\n",
      " [8 2 8]\n",
      " [7 8 2]\n",
      " [2 7 7]\n",
      " [9 9 9]\n",
      " [1 1 1]]\n",
      "\n",
      "batch 1600\n",
      "  minibatch loss: 0.32577282190322876\n",
      "  sample 1:\n",
      "    input     > [2 9 7 3 5 3 6 7]\n",
      "    predicted > [[9 9 9]\n",
      " [2 5 7]\n",
      " [7 6 2]\n",
      " [3 7 3]\n",
      " [7 3 6]\n",
      " [6 2 5]\n",
      " [5 2 3]\n",
      " [3 3 4]\n",
      " [1 1 1]]\n",
      "  sample 2:\n",
      "    input     > [4 3 9 5 0 0 0 0]\n",
      "    predicted > [[ 4  4  4]\n",
      " [ 3  3  9]\n",
      " [ 9  9  3]\n",
      " [ 5  9  3]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 3:\n",
      "    input     > [5 7 9 8 5 0 0 0]\n",
      "    predicted > [[ 5  5  5]\n",
      " [ 7  7  5]\n",
      " [ 9  8  3]\n",
      " [ 8  9  8]\n",
      " [ 5  5  7]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "\n",
      "batch 1800\n",
      "  minibatch loss: 0.36575061082839966\n",
      "  sample 1:\n",
      "    input     > [2 7 4 3 4 6 5 0]\n",
      "    predicted > [[ 2  2  2]\n",
      " [ 7  4  7]\n",
      " [ 4  7  4]\n",
      " [ 3  3  6]\n",
      " [ 4  6  9]\n",
      " [ 6  5  3]\n",
      " [ 5  4  5]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]]\n",
      "  sample 2:\n",
      "    input     > [9 4 4 0 0 0 0 0]\n",
      "    predicted > [[ 9  4  9]\n",
      " [ 4  9  4]\n",
      " [ 4  9  2]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 3:\n",
      "    input     > [7 3 3 2 9 4 7 4]\n",
      "    predicted > [[ 3  3  3]\n",
      " [ 7  7  7]\n",
      " [ 3  2  2]\n",
      " [ 2  3  3]\n",
      " [ 9  9  9]\n",
      " [ 4  4  4]\n",
      " [ 7  7  7]\n",
      " [ 4  4  1]\n",
      " [ 1  1 -1]]\n",
      "\n",
      "batch 2000\n",
      "  minibatch loss: 0.19473139941692352\n",
      "  sample 1:\n",
      "    input     > [8 6 3 0 0 0 0]\n",
      "    predicted > [[ 8  8  6]\n",
      " [ 6  3  8]\n",
      " [ 3  6  3]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 2:\n",
      "    input     > [6 4 4 8 5 2 0]\n",
      "    predicted > [[ 6  6  4]\n",
      " [ 4  4  6]\n",
      " [ 4  4  4]\n",
      " [ 8  8  8]\n",
      " [ 5  2  2]\n",
      " [ 2  5  5]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]]\n",
      "  sample 3:\n",
      "    input     > [7 2 7 6 0 0 0]\n",
      "    predicted > [[ 7  7  7]\n",
      " [ 2  7  7]\n",
      " [ 7  2  2]\n",
      " [ 6  6  4]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "\n",
      "batch 2200\n",
      "  minibatch loss: 0.22542116045951843\n",
      "  sample 1:\n",
      "    input     > [7 3 4 0 0 0 0 0]\n",
      "    predicted > [[ 7  3  7]\n",
      " [ 3  7  6]\n",
      " [ 4  4  9]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 2:\n",
      "    input     > [3 9 7 3 0 0 0 0]\n",
      "    predicted > [[ 3  3  3]\n",
      " [ 9  3  7]\n",
      " [ 7  9  9]\n",
      " [ 3  7  3]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 3:\n",
      "    input     > [9 7 6 7 6 3 2 8]\n",
      "    predicted > [[7 7 9]\n",
      " [9 9 7]\n",
      " [6 6 6]\n",
      " [3 3 7]\n",
      " [2 2 6]\n",
      " [8 6 3]\n",
      " [6 8 8]\n",
      " [7 7 2]\n",
      " [1 1 1]]\n",
      "\n",
      "batch 2400\n",
      "  minibatch loss: 0.236276313662529\n",
      "  sample 1:\n",
      "    input     > [5 3 7 8 7 3 0 0]\n",
      "    predicted > [[ 5  5  5]\n",
      " [ 3  7  7]\n",
      " [ 7  3  3]\n",
      " [ 8  8  8]\n",
      " [ 7  3  3]\n",
      " [ 3  7  3]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 2:\n",
      "    input     > [4 8 3 5 9 3 0 0]\n",
      "    predicted > [[ 4  4  4]\n",
      " [ 8  8  8]\n",
      " [ 3  9  5]\n",
      " [ 5  3  3]\n",
      " [ 9  5  3]\n",
      " [ 3  3  9]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 3:\n",
      "    input     > [8 2 4 8 7 2 0 0]\n",
      "    predicted > [[ 8  8  8]\n",
      " [ 2  2  4]\n",
      " [ 8  4  2]\n",
      " [ 4  8  2]\n",
      " [ 2  7  8]\n",
      " [ 7  2  7]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "\n",
      "batch 2600\n",
      "  minibatch loss: 0.18354903161525726\n",
      "  sample 1:\n",
      "    input     > [2 8 5 5 3 0 0 0]\n",
      "    predicted > [[ 2  4  2]\n",
      " [ 8  5  5]\n",
      " [ 5  2  8]\n",
      " [ 5  3  8]\n",
      " [ 3  8  5]\n",
      " [ 1  5  3]\n",
      " [-1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 2:\n",
      "    input     > [6 8 5 5 5 0 0 0]\n",
      "    predicted > [[ 6  6  6]\n",
      " [ 8  5  8]\n",
      " [ 5  8  5]\n",
      " [ 5  8  5]\n",
      " [ 5  5  2]\n",
      " [ 1  5  1]\n",
      " [-1  1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 3:\n",
      "    input     > [6 2 6 4 2 4 6 3]\n",
      "    predicted > [[6 6 6]\n",
      " [2 2 2]\n",
      " [6 4 4]\n",
      " [4 6 6]\n",
      " [4 6 6]\n",
      " [2 2 4]\n",
      " [6 4 2]\n",
      " [3 9 9]\n",
      " [1 1 1]]\n",
      "\n",
      "batch 2800\n",
      "  minibatch loss: 0.20125198364257812\n",
      "  sample 1:\n",
      "    input     > [9 7 6 2 6 3 0 0]\n",
      "    predicted > [[ 9  2  2]\n",
      " [ 7  9  9]\n",
      " [ 6  6  7]\n",
      " [ 2  7  6]\n",
      " [ 6  3  6]\n",
      " [ 3  6  3]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 2:\n",
      "    input     > [9 7 5 6 5 9 6 0]\n",
      "    predicted > [[ 9  5  5]\n",
      " [ 7  9  9]\n",
      " [ 5  6  6]\n",
      " [ 6  7  7]\n",
      " [ 5  9  9]\n",
      " [ 2  5  7]\n",
      " [ 6  6  9]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]]\n",
      "  sample 3:\n",
      "    input     > [9 2 2 0 0 0 0 0]\n",
      "    predicted > [[ 9  2  9]\n",
      " [ 2  9  2]\n",
      " [ 2  9  9]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "\n",
      "batch 3000\n",
      "  minibatch loss: 0.14697885513305664\n",
      "  sample 1:\n",
      "    input     > [6 2 3 0 0 0 0]\n",
      "    predicted > [[ 6  6  6]\n",
      " [ 2  3  2]\n",
      " [ 3  2  9]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 2:\n",
      "    input     > [3 9 8 4 9 2 2]\n",
      "    predicted > [[3 3 3]\n",
      " [9 8 9]\n",
      " [8 9 8]\n",
      " [4 9 9]\n",
      " [9 4 4]\n",
      " [2 2 2]\n",
      " [2 2 2]\n",
      " [1 1 1]]\n",
      "  sample 3:\n",
      "    input     > [7 7 3 2 0 0 0]\n",
      "    predicted > [[ 7  7  7]\n",
      " [ 7  3  3]\n",
      " [ 3  7  7]\n",
      " [ 2  2  7]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "\n",
      "batch 3200\n",
      "  minibatch loss: 0.19483646750450134\n",
      "  sample 1:\n",
      "    input     > [3 7 8 8 8 6 0 0]\n",
      "    predicted > [[ 3  3  3]\n",
      " [ 7  8  8]\n",
      " [ 8  7  7]\n",
      " [ 8  8  8]\n",
      " [ 8  6  6]\n",
      " [ 6  8  5]\n",
      " [ 1  1  8]\n",
      " [-1 -1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 2:\n",
      "    input     > [6 2 5 8 5 3 3 3]\n",
      "    predicted > [[ 6  6  6]\n",
      " [ 5  5  5]\n",
      " [ 2  4  8]\n",
      " [ 8  2  2]\n",
      " [ 3  3  3]\n",
      " [ 5  3  2]\n",
      " [ 3  5  5]\n",
      " [ 2  8  3]\n",
      " [ 1  3  1]\n",
      " [-1  1 -1]]\n",
      "  sample 3:\n",
      "    input     > [6 4 4 4 3 2 8 0]\n",
      "    predicted > [[ 6  6  6]\n",
      " [ 4  4  4]\n",
      " [ 4  4  4]\n",
      " [ 4  4  4]\n",
      " [ 3  3  3]\n",
      " [ 2  8  4]\n",
      " [ 8  2  2]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "\n",
      "batch 3400\n",
      "  minibatch loss: 0.14759384095668793\n",
      "  sample 1:\n",
      "    input     > [2 2 6 0 0 0 0 0]\n",
      "    predicted > [[ 2  2  2]\n",
      " [ 2  2  2]\n",
      " [ 6  3  3]\n",
      " [ 1  1  6]\n",
      " [-1 -1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 2:\n",
      "    input     > [5 6 6 2 8 5 0 0]\n",
      "    predicted > [[ 5  2  2]\n",
      " [ 6  6  8]\n",
      " [ 6  5  6]\n",
      " [ 2  8  5]\n",
      " [ 8  6  6]\n",
      " [ 5  8  5]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 3:\n",
      "    input     > [6 9 2 0 0 0 0 0]\n",
      "    predicted > [[ 6  6  6]\n",
      " [ 9  2  9]\n",
      " [ 2  9  9]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "\n",
      "batch 3600\n",
      "  minibatch loss: 0.13171222805976868\n",
      "  sample 1:\n",
      "    input     > [2 6 3 9 3 7 7 0]\n",
      "    predicted > [[ 2  6  6]\n",
      " [ 6  9  9]\n",
      " [ 3  2  2]\n",
      " [ 9  3  3]\n",
      " [ 3  7  7]\n",
      " [ 7  3  6]\n",
      " [ 7  7  7]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]]\n",
      "  sample 2:\n",
      "    input     > [3 6 3 3 5 3 6 0]\n",
      "    predicted > [[ 3  3  3]\n",
      " [ 6  3  3]\n",
      " [ 3  6  6]\n",
      " [ 3  6  3]\n",
      " [ 5  5  4]\n",
      " [ 3  3  7]\n",
      " [ 6  3  3]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]]\n",
      "  sample 3:\n",
      "    input     > [2 2 8 9 6 5 5 7]\n",
      "    predicted > [[2 2 2]\n",
      " [2 2 2]\n",
      " [8 8 8]\n",
      " [9 9 6]\n",
      " [6 6 9]\n",
      " [5 5 5]\n",
      " [5 7 5]\n",
      " [7 5 7]\n",
      " [1 1 1]]\n",
      "\n",
      "batch 3800\n",
      "  minibatch loss: 0.058824554085731506\n",
      "  sample 1:\n",
      "    input     > [7 9 9 8 7 7 4 3]\n",
      "    predicted > [[7 7 9]\n",
      " [9 9 7]\n",
      " [9 9 7]\n",
      " [8 7 8]\n",
      " [7 8 9]\n",
      " [7 4 7]\n",
      " [4 3 4]\n",
      " [3 7 3]\n",
      " [1 1 1]]\n",
      "  sample 2:\n",
      "    input     > [6 4 3 9 5 3 0 0]\n",
      "    predicted > [[ 6  6  6]\n",
      " [ 4  4  4]\n",
      " [ 3  3  3]\n",
      " [ 9  9  5]\n",
      " [ 5  3  9]\n",
      " [ 3  5  3]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 3:\n",
      "    input     > [8 9 9 7 0 0 0 0]\n",
      "    predicted > [[ 8  9  9]\n",
      " [ 9  8  8]\n",
      " [ 9  8  8]\n",
      " [ 7  7  7]\n",
      " [ 1  9  1]\n",
      " [-1  1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "\n",
      "batch 4000\n",
      "  minibatch loss: 0.09603714197874069\n",
      "  sample 1:\n",
      "    input     > [6 3 4 3 7 3 0 0]\n",
      "    predicted > [[ 6  6  6]\n",
      " [ 3  3  3]\n",
      " [ 4  4  4]\n",
      " [ 3  7  3]\n",
      " [ 7  3  3]\n",
      " [ 3  3  7]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 2:\n",
      "    input     > [8 3 3 6 0 0 0 0]\n",
      "    predicted > [[ 8  8  3]\n",
      " [ 3  3  8]\n",
      " [ 3  6  8]\n",
      " [ 6  3  6]\n",
      " [ 1  1  3]\n",
      " [-1 -1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 3:\n",
      "    input     > [2 6 4 8 9 9 2 3]\n",
      "    predicted > [[2 2 2]\n",
      " [6 6 6]\n",
      " [4 8 4]\n",
      " [8 4 9]\n",
      " [9 9 8]\n",
      " [9 2 9]\n",
      " [2 9 3]\n",
      " [3 3 2]\n",
      " [1 1 1]]\n",
      "\n",
      "batch 4200\n",
      "  minibatch loss: 0.18101732432842255\n",
      "  sample 1:\n",
      "    input     > [5 4 8 9 8 5 5 0]\n",
      "    predicted > [[ 5  5  5]\n",
      " [ 4  4  4]\n",
      " [ 8  8  8]\n",
      " [ 9  9  9]\n",
      " [ 8  5  4]\n",
      " [ 5  8  5]\n",
      " [ 5  5  8]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]]\n",
      "  sample 2:\n",
      "    input     > [2 6 4 8 8 3 0 0]\n",
      "    predicted > [[ 2  2  2]\n",
      " [ 6  6  6]\n",
      " [ 4  4  8]\n",
      " [ 8  8  4]\n",
      " [ 8  3  8]\n",
      " [ 3  8  3]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 3:\n",
      "    input     > [6 6 7 2 6 4 6 0]\n",
      "    predicted > [[ 6  6  6]\n",
      " [ 6  6  6]\n",
      " [ 7  7  7]\n",
      " [ 2  2  6]\n",
      " [ 6  6  2]\n",
      " [ 6  4  4]\n",
      " [ 4  6  6]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]]\n",
      "\n",
      "batch 4400\n",
      "  minibatch loss: 0.13958677649497986\n",
      "  sample 1:\n",
      "    input     > [4 2 9 4 9 7 2 2]\n",
      "    predicted > [[4 4 4]\n",
      " [2 9 9]\n",
      " [9 2 2]\n",
      " [9 4 4]\n",
      " [4 7 2]\n",
      " [7 2 7]\n",
      " [2 9 9]\n",
      " [2 2 2]\n",
      " [1 1 1]]\n",
      "  sample 2:\n",
      "    input     > [5 6 7 2 9 2 2 6]\n",
      "    predicted > [[5 5 5]\n",
      " [6 6 6]\n",
      " [7 2 7]\n",
      " [2 7 2]\n",
      " [9 9 9]\n",
      " [2 7 2]\n",
      " [2 2 6]\n",
      " [6 6 2]\n",
      " [1 1 1]]\n",
      "  sample 3:\n",
      "    input     > [2 7 5 8 9 3 5 0]\n",
      "    predicted > [[ 2  2  2]\n",
      " [ 7  7  7]\n",
      " [ 5  5  8]\n",
      " [ 8  9  5]\n",
      " [ 9  8  9]\n",
      " [ 3  3  5]\n",
      " [ 5  5  3]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]]\n",
      "\n",
      "batch 4600\n",
      "  minibatch loss: 0.06325320154428482\n",
      "  sample 1:\n",
      "    input     > [3 5 7 9 0 0 0]\n",
      "    predicted > [[ 3  7  3]\n",
      " [ 5  3  7]\n",
      " [ 7  9  5]\n",
      " [ 9  5  9]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 2:\n",
      "    input     > [9 6 8 2 0 0 0]\n",
      "    predicted > [[ 9  9  9]\n",
      " [ 6  8  6]\n",
      " [ 8  6  6]\n",
      " [ 2  2  5]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 3:\n",
      "    input     > [9 9 6 0 0 0 0]\n",
      "    predicted > [[ 9  9  9]\n",
      " [ 9  9  9]\n",
      " [ 6  6  6]\n",
      " [ 1  2  6]\n",
      " [-1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "\n",
      "batch 4800\n",
      "  minibatch loss: 0.08858782052993774\n",
      "  sample 1:\n",
      "    input     > [6 5 3 0 0 0 0 0]\n",
      "    predicted > [[ 6  6  6]\n",
      " [ 5  5  7]\n",
      " [ 3  6  5]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 2:\n",
      "    input     > [4 9 4 6 9 4 0 0]\n",
      "    predicted > [[ 4  4  4]\n",
      " [ 9  9  9]\n",
      " [ 4  4  6]\n",
      " [ 6  6  4]\n",
      " [ 9  4  9]\n",
      " [ 4  9  4]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 3:\n",
      "    input     > [5 5 4 9 9 0 0 0]\n",
      "    predicted > [[ 5  5  5]\n",
      " [ 5  5  5]\n",
      " [ 4  4  4]\n",
      " [ 9  9  9]\n",
      " [ 9  5  4]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "\n",
      "batch 5000\n",
      "  minibatch loss: 0.07043668627738953\n",
      "  sample 1:\n",
      "    input     > [3 5 3 8 4 7 4 0]\n",
      "    predicted > [[ 3  3  3]\n",
      " [ 5  5  5]\n",
      " [ 3  3  3]\n",
      " [ 8  8  4]\n",
      " [ 4  4  8]\n",
      " [ 7  4  7]\n",
      " [ 4  7  8]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]]\n",
      "  sample 2:\n",
      "    input     > [4 9 2 2 5 4 0 0]\n",
      "    predicted > [[ 4  4  4]\n",
      " [ 9  2  2]\n",
      " [ 2  9  9]\n",
      " [ 2  5  9]\n",
      " [ 5  4  5]\n",
      " [ 4  2  4]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 3:\n",
      "    input     > [4 4 9 7 3 4 9 4]\n",
      "    predicted > [[4 4 4]\n",
      " [4 4 4]\n",
      " [9 7 9]\n",
      " [7 9 7]\n",
      " [3 9 3]\n",
      " [4 4 4]\n",
      " [9 3 4]\n",
      " [4 4 9]\n",
      " [1 1 1]]\n",
      "\n",
      "batch 5200\n",
      "  minibatch loss: 0.09076255559921265\n",
      "  sample 1:\n",
      "    input     > [7 6 9 2 5 6 3 0]\n",
      "    predicted > [[ 7  7  7]\n",
      " [ 6  6  2]\n",
      " [ 9  9  6]\n",
      " [ 2  2  9]\n",
      " [ 5  5  8]\n",
      " [ 6  6  3]\n",
      " [ 3  7  5]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 2:\n",
      "    input     > [5 4 9 4 2 3 3 0]\n",
      "    predicted > [[ 5  5  4]\n",
      " [ 4  4  5]\n",
      " [ 9  9  9]\n",
      " [ 4  2  2]\n",
      " [ 2  4  9]\n",
      " [ 3  3  6]\n",
      " [ 3  3  3]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 3:\n",
      "    input     > [6 6 5 6 2 6 8 2]\n",
      "    predicted > [[ 6  6  6]\n",
      " [ 6  6  3]\n",
      " [ 5  5  6]\n",
      " [ 6  6  5]\n",
      " [ 2  2  6]\n",
      " [ 6  6  4]\n",
      " [ 8  8  2]\n",
      " [ 2  6  6]\n",
      " [ 1  1  5]\n",
      " [-1 -1  1]]\n",
      "\n",
      "batch 5400\n",
      "  minibatch loss: 0.06568838655948639\n",
      "  sample 1:\n",
      "    input     > [5 6 3 8 9 2 0 0]\n",
      "    predicted > [[ 5  5  5]\n",
      " [ 6  6  3]\n",
      " [ 3  3  6]\n",
      " [ 8  8  8]\n",
      " [ 9  2  4]\n",
      " [ 2  9  9]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 2:\n",
      "    input     > [6 2 4 3 9 7 8 8]\n",
      "    predicted > [[6 6 6]\n",
      " [2 2 2]\n",
      " [4 4 3]\n",
      " [3 3 4]\n",
      " [9 9 4]\n",
      " [7 8 9]\n",
      " [8 7 7]\n",
      " [8 8 8]\n",
      " [1 1 1]]\n",
      "  sample 3:\n",
      "    input     > [4 5 4 4 5 5 8 0]\n",
      "    predicted > [[ 4  4  4]\n",
      " [ 5  4  4]\n",
      " [ 4  5  5]\n",
      " [ 4  5  5]\n",
      " [ 5  5  4]\n",
      " [ 5  4  5]\n",
      " [ 8  8  8]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]]\n",
      "\n",
      "batch 5600\n",
      "  minibatch loss: 0.10547281056642532\n",
      "  sample 1:\n",
      "    input     > [5 9 5 4 9 0 0 0]\n",
      "    predicted > [[ 5  5  5]\n",
      " [ 9  5  9]\n",
      " [ 5  9  5]\n",
      " [ 4  4  4]\n",
      " [ 9  9  4]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 2:\n",
      "    input     > [5 9 2 0 0 0 0 0]\n",
      "    predicted > [[ 5  9  5]\n",
      " [ 9  5  2]\n",
      " [ 2  2  9]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 3:\n",
      "    input     > [2 3 6 5 7 8 5 2]\n",
      "    predicted > [[2 2 2]\n",
      " [3 3 3]\n",
      " [6 6 6]\n",
      " [5 5 5]\n",
      " [7 7 8]\n",
      " [8 8 7]\n",
      " [5 2 5]\n",
      " [2 5 2]\n",
      " [1 1 1]]\n",
      "\n",
      "batch 5800\n",
      "  minibatch loss: 0.033192142844200134\n",
      "  sample 1:\n",
      "    input     > [3 5 4 6 8 0 0]\n",
      "    predicted > [[ 3  3  3]\n",
      " [ 5  5  5]\n",
      " [ 4  6  6]\n",
      " [ 6  4  4]\n",
      " [ 8  5  8]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 2:\n",
      "    input     > [4 4 3 2 5 8 0]\n",
      "    predicted > [[ 4  4  4]\n",
      " [ 4  4  2]\n",
      " [ 3  3  4]\n",
      " [ 2  2  3]\n",
      " [ 5  5  8]\n",
      " [ 8  4  5]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]]\n",
      "  sample 3:\n",
      "    input     > [9 6 2 0 0 0 0]\n",
      "    predicted > [[ 9  9  2]\n",
      " [ 6  2  9]\n",
      " [ 2  6  6]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "\n",
      "batch 6000\n",
      "  minibatch loss: 0.05001354217529297\n",
      "  sample 1:\n",
      "    input     > [3 8 5 7 0 0 0 0]\n",
      "    predicted > [[ 3  3  3]\n",
      " [ 8  8  8]\n",
      " [ 5  7  5]\n",
      " [ 7  5  3]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 2:\n",
      "    input     > [5 2 9 3 4 3 0 0]\n",
      "    predicted > [[ 5  5  2]\n",
      " [ 2  2  5]\n",
      " [ 9  3  9]\n",
      " [ 3  9  3]\n",
      " [ 4  4  4]\n",
      " [ 3  3  3]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 3:\n",
      "    input     > [4 9 7 6 0 0 0 0]\n",
      "    predicted > [[ 4  4  4]\n",
      " [ 9  9  9]\n",
      " [ 7  6  3]\n",
      " [ 6  7  7]\n",
      " [ 1  1  6]\n",
      " [-1 -1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss_track = []\n",
    "max_batches = 6001\n",
    "batches_in_epoch = 200\n",
    "\n",
    "try:\n",
    "    # 一个epoch的learning\n",
    "    for batch in range(max_batches):\n",
    "        fd = next_feed()\n",
    "        _, l = sess.run([train_op, loss], fd)\n",
    "        loss_track.append(l)\n",
    "        \n",
    "        if batch == 0 or batch % batches_in_epoch == 0:\n",
    "            print('batch {}'.format(batch))\n",
    "            print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
    "            predict_ = sess.run(beam_decoder_outputs.predicted_ids, fd)\n",
    "            #print(predict_)\n",
    "            for i, (inp, pred) in enumerate(zip(fd[encoder_inputs], predict_)):\n",
    "                print('  sample {}:'.format(i + 1))\n",
    "                print('    input     > {}'.format(inp))\n",
    "                print('    predicted > {}'.format(pred))\n",
    "                if i >= 2:\n",
    "                    break\n",
    "            print()\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print('training interrupted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.0543 after 60010 examples (batch_size=10)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_track)\n",
    "print('loss {:.4f} after {} examples (batch_size={})'.format(loss_track[-1], \n",
    "                                                             len(loss_track)*batch_size, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
