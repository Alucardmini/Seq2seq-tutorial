{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实现多层双向的dynamic_lstm+beam_search\n",
    "### 基于tensorflow1.4 Seq2seq的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encoder使用的两层双向的LSTM，注意multi_RNN与bi_dynamic_lstm（并不兼容）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "产生10个长度不一（最短3，最长8）的sequences, 其中前十个是:\n",
      "[6, 6, 3, 9, 7, 7, 9, 4]\n",
      "[9, 3, 6, 3, 6, 6, 4, 5]\n",
      "[5, 4, 2, 2, 3, 9, 8, 7]\n",
      "[3, 2, 7]\n",
      "[8, 5, 9, 4, 5, 2]\n",
      "[6, 5, 8, 9, 4]\n",
      "[3, 9, 6, 5, 2, 2]\n",
      "[3, 2, 2, 3]\n",
      "[8, 8, 7, 6, 8]\n",
      "[5, 3, 3, 6, 8, 7, 4, 9]\n"
     ]
    }
   ],
   "source": [
    "import helpers\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.util import nest\n",
    "from tensorflow.contrib import seq2seq,rnn\n",
    "\n",
    "tf.__version__\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "PAD = 0\n",
    "EOS = 1\n",
    "\n",
    "\n",
    "vocab_size = 10\n",
    "input_embedding_size = 20\n",
    "encoder_hidden_units = 25\n",
    "\n",
    "decoder_hidden_units = encoder_hidden_units\n",
    "\n",
    "import helpers as data_helpers\n",
    "batch_size = 10\n",
    "\n",
    "# 一个generator，每次产生一个minibatch的随机样本\n",
    "\n",
    "batches = data_helpers.random_sequences(length_from=3, length_to=8,\n",
    "                                   vocab_lower=2, vocab_upper=10,\n",
    "                                   batch_size=batch_size)\n",
    "\n",
    "print('产生%d个长度不一（最短3，最长8）的sequences, 其中前十个是:' % batch_size)\n",
    "for seq in next(batches)[:min(batch_size, 10)]:\n",
    "    print(seq)\n",
    "    \n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "mode = tf.contrib.learn.ModeKeys.TRAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.使用seq2seq库实现seq2seq模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('minibatch'):\n",
    "    encoder_inputs = tf.placeholder(tf.int32, [None, None], name='encoder_inputs')\n",
    "    \n",
    "    encoder_inputs_length = tf.placeholder(tf.int32, [None], name='encoder_inputs_length')\n",
    "    \n",
    "    decoder_targets = tf.placeholder(tf.int32, [None, None], name='decoder_targets')\n",
    "    \n",
    "    decoder_inputs = tf.placeholder(shape=(None, None),dtype=tf.int32,name='decoder_inputs')\n",
    "    \n",
    "    #decoder_inputs_length和decoder_targets_length是一样的\n",
    "    decoder_inputs_length = tf.placeholder(shape=(None,),\n",
    "                                            dtype=tf.int32,\n",
    "                                            name='decoder_inputs_length')\n",
    "    \n",
    "# 构建embedding矩阵,encoder和decoder公用该词向量矩阵\n",
    "embedding = tf.get_variable('embedding', [vocab_size,input_embedding_size])\n",
    "encoder_inputs_embedded = tf.nn.embedding_lookup(embedding,encoder_inputs)\n",
    "\n",
    "#fw_cell = bw_cell =  rnn.LSTMCell(encoder_hidden_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义encoder，两层双向lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_inputs=encoder_inputs_embedded\n",
    "for _ in range(2):\n",
    "    #为什么在这加个variable_scope,被逼的,tf在rnn_cell的__call__中非要搞一个命名空间检查\n",
    "    #恶心的很.如果不在这加的话,会报错的.\n",
    "    with tf.variable_scope(None, default_name=\"bidirectional-rnn\"):\n",
    "        rnn_cell_bw =  rnn_cell_fw = rnn.LSTMCell(encoder_hidden_units)\n",
    "        #rnn_cell_bw = rnn.LSTMCell(encoder_hidden_units)\n",
    "        #initial_state_fw = rnn_cell_fw.zero_state(batch_size, dtype=tf.float32)\n",
    "        #initial_state_bw = rnn_cell_bw.zero_state(batch_size, dtype=tf.float32)\n",
    "        ((encoder_fw_outputs,encoder_bw_outputs),(encoder_fw_final_state,encoder_bw_final_state))\\\n",
    "        = tf.nn.bidirectional_dynamic_rnn(cell_fw=rnn_cell_fw,\n",
    "                                              cell_bw=rnn_cell_bw, \n",
    "                                              inputs=_inputs, \n",
    "                                              sequence_length=encoder_inputs_length,\n",
    "                                              dtype=tf.float32)\n",
    "        _inputs = tf.concat((encoder_fw_outputs,encoder_bw_outputs), 2)\n",
    "#取最后一层的 final_state    \n",
    "encoder_final_state_h = tf.concat((encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n",
    "encoder_final_state_c = tf.concat((encoder_fw_final_state.c, encoder_bw_final_state.c), 1)\n",
    "encoder_final_state = rnn.LSTMStateTuple(c=encoder_final_state_c, h=encoder_final_state_h)\n",
    "encoder_final_output = _inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMStateTuple(c=<tf.Tensor 'concat_3:0' shape=(?, 50) dtype=float32>, h=<tf.Tensor 'concat_2:0' shape=(?, 50) dtype=float32>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    encoder_final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'bidirectional-rnn_4/concat:0' shape=(?, ?, 50) dtype=float32>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    encoder_final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.定义decoder 部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t<tf.Tensor 'decoder/Identity:0' shape=(?, ?, 10) dtype=float32>\n",
      "\t<tf.Tensor 'minibatch/decoder_targets:0' shape=(?, ?) dtype=int32>\n",
      "\t<tf.Tensor 'decoder/decoder/transpose_1:0' shape=(?, ?) dtype=int32>\n"
     ]
    }
   ],
   "source": [
    "def _create_rnn_cell2():\n",
    "    def single_rnn_cell(encoder_hidden_units):\n",
    "        # 创建单个cell，这里需要注意的是一定要使用一个single_rnn_cell的函数，不然直接把cell放在MultiRNNCell\n",
    "        # 的列表中最终模型会发生错误\n",
    "        single_cell = rnn.LSTMCell(encoder_hidden_units*2)\n",
    "        #添加dropout\n",
    "        single_cell = rnn.DropoutWrapper(single_cell, output_keep_prob=0.5)\n",
    "        return single_cell\n",
    "            #列表中每个元素都是调用single_rnn_cell函数\n",
    "            #cell = rnn.MultiRNNCell([single_rnn_cell() for _ in range(self.num_layers)])\n",
    "    cell = rnn.MultiRNNCell([single_rnn_cell(encoder_hidden_units) for _ in range(1)])\n",
    "    return cell \n",
    "\n",
    "with tf.variable_scope('decoder'):\n",
    "    #single_cell = rnn.LSTMCell(encoder_hidden_units)\n",
    "    #decoder_cell = rnn.MultiRNNCell([single_cell for _ in range(1)])\n",
    "    decoder_cell = rnn.LSTMCell(encoder_hidden_units*2)\n",
    "    #定义decoder的初始状态\n",
    "    decoder_initial_state = encoder_final_state\n",
    "    \n",
    "    #定义output_layer\n",
    "    output_layer = tf.layers.Dense(vocab_size,kernel_initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.1))\n",
    "    \n",
    "    decoder_inputs_embedded = tf.nn.embedding_lookup(embedding, decoder_inputs)\n",
    "    \n",
    "    # 训练阶段，使用TrainingHelper+BasicDecoder的组合，这一般是固定的，当然也可以自己定义Helper类，实现自己的功能\n",
    "    training_helper = seq2seq.TrainingHelper(inputs=decoder_inputs_embedded,\n",
    "                                                        sequence_length=decoder_inputs_length,\n",
    "                                                        time_major=False, name='training_helper')\n",
    "    training_decoder = seq2seq.BasicDecoder(cell=decoder_cell, helper=training_helper,\n",
    "                                                       initial_state=decoder_initial_state,\n",
    "                                                       output_layer=output_layer)\n",
    "    \n",
    "    # 调用dynamic_decode进行解码，decoder_outputs是一个namedtuple，里面包含两项(rnn_outputs, sample_id)\n",
    "    # rnn_output: [batch_size, decoder_targets_length, vocab_size]，保存decode每个时刻每个单词的概率，可以用来计算loss\n",
    "    # sample_id: [batch_size], tf.int32，保存最终的编码结果。可以表示最后的答案\n",
    "    max_target_sequence_length = tf.reduce_max(decoder_inputs_length, name='max_target_len')\n",
    "    decoder_outputs, _, _ = seq2seq.dynamic_decode(decoder=training_decoder,\n",
    "                                                          impute_finished=True,\n",
    "                                                          maximum_iterations=max_target_sequence_length)\n",
    "    decoder_logits_train = tf.identity(decoder_outputs.rnn_output)\n",
    "    sample_id = decoder_outputs.sample_id\n",
    "    max_target_sequence_length = tf.reduce_max(decoder_inputs_length, name='max_target_len')\n",
    "    mask = tf.sequence_mask(decoder_inputs_length,max_target_sequence_length, dtype=tf.float32, name='masks')\n",
    "    print('\\t%s' % repr(decoder_logits_train))\n",
    "    print('\\t%s' % repr(decoder_targets))\n",
    "    print('\\t%s' % repr(sample_id))\n",
    "    loss = seq2seq.sequence_loss(logits=decoder_logits_train,targets=decoder_targets, weights=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('decoder',reuse=True):\n",
    "    start_tokens = tf.ones([batch_size, ], tf.int32)*1  #[batch_size]  数值为1\n",
    "    encoder_state = nest.map_structure(lambda s: seq2seq.tile_batch(s, 3),\n",
    "                                                   encoder_final_state)\n",
    "    inference_decoder = tf.contrib.seq2seq.BeamSearchDecoder(cell=decoder_cell, embedding=embedding,\n",
    "                                                                             start_tokens=start_tokens,\n",
    "                                                                             end_token=1,\n",
    "                                                                             initial_state=encoder_state,\n",
    "                                                                             beam_width=3,\n",
    "                                                                             output_layer=output_layer)\n",
    "    beam_decoder_outputs, _, _ = seq2seq.dynamic_decode(decoder=inference_decoder,maximum_iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_inputs:\n",
      "[3 3 7 3 0 0 0 0]\n",
      "encoder_inputs_length:\n",
      "4\n",
      "decoder_inputs:\n",
      "[1 3 3 7 3 0 0 0 0]\n",
      "decoder_inputs_length:\n",
      "5\n",
      "decoder_targets:\n",
      "[3 3 7 3 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "train_op = tf.train.AdamOptimizer(learning_rate = 0.001).minimize(loss)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "def next_feed():\n",
    "    batch = next(batches)\n",
    "    \n",
    "    encoder_inputs_, encoder_inputs_length_ = data_helpers.batch(batch)\n",
    "    decoder_targets_, decoder_targets_length_ = data_helpers.batch(\n",
    "        [(sequence) + [EOS] for sequence in batch]\n",
    "    )\n",
    "    decoder_inputs_, decoder_inputs_length_ = data_helpers.batch(\n",
    "        [[EOS] + (sequence) for sequence in batch]\n",
    "    )\n",
    "    \n",
    "    # 在feedDict里面，key可以是一个Tensor\n",
    "    return {\n",
    "        encoder_inputs: encoder_inputs_.T,\n",
    "        decoder_inputs: decoder_inputs_.T,\n",
    "        decoder_targets: decoder_targets_.T,\n",
    "        encoder_inputs_length: encoder_inputs_length_,\n",
    "        decoder_inputs_length: decoder_inputs_length_\n",
    "    }\n",
    "\n",
    "x = next_feed()\n",
    "print('encoder_inputs:')\n",
    "print(x[encoder_inputs][0,:])\n",
    "print('encoder_inputs_length:')\n",
    "print(x[encoder_inputs_length][0])\n",
    "print('decoder_inputs:')\n",
    "print(x[decoder_inputs][0,:])\n",
    "print('decoder_inputs_length:')\n",
    "print(x[decoder_inputs_length][0])\n",
    "print('decoder_targets:')\n",
    "print(x[decoder_targets][0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "  minibatch loss: 2.2935664653778076\n",
      "  sample 1:\n",
      "    input     > [9 2 8 0 0 0 0 0]\n",
      "    predicted > [[5 5 5]\n",
      " [5 5 5]\n",
      " [5 5 5]\n",
      " [5 5 8]\n",
      " [8 5 8]\n",
      " [8 8 8]\n",
      " [8 8 8]\n",
      " [8 8 8]\n",
      " [8 8 8]\n",
      " [8 8 8]]\n",
      "  sample 2:\n",
      "    input     > [6 8 9 7 2 6 9 3]\n",
      "    predicted > [[5 5 5]\n",
      " [5 5 5]\n",
      " [5 5 8]\n",
      " [8 8 8]\n",
      " [8 8 8]\n",
      " [8 8 8]\n",
      " [8 8 8]\n",
      " [8 8 8]\n",
      " [8 8 8]\n",
      " [8 9 8]]\n",
      "  sample 3:\n",
      "    input     > [3 6 6 0 0 0 0 0]\n",
      "    predicted > [[5 5 5]\n",
      " [5 5 5]\n",
      " [5 5 8]\n",
      " [8 8 8]\n",
      " [8 8 8]\n",
      " [8 8 8]\n",
      " [8 8 8]\n",
      " [8 8 8]\n",
      " [8 8 8]\n",
      " [8 9 8]]\n",
      "\n",
      "batch 200\n",
      "  minibatch loss: 1.4949365854263306\n",
      "  sample 1:\n",
      "    input     > [4 7 8 6 7 9 0 0]\n",
      "    predicted > [[ 3  3  4]\n",
      " [ 4  4  3]\n",
      " [ 5  5  5]\n",
      " [ 7  5  5]\n",
      " [ 4  9  9]\n",
      " [ 9  4  4]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 2:\n",
      "    input     > [9 3 7 4 0 0 0 0]\n",
      "    predicted > [[ 4  4  4]\n",
      " [ 9  9  9]\n",
      " [ 5  4  9]\n",
      " [ 4  4  4]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 3:\n",
      "    input     > [2 5 9 6 0 0 0 0]\n",
      "    predicted > [[ 9  6  9]\n",
      " [ 6  9  6]\n",
      " [ 6  2  6]\n",
      " [ 1  1  2]\n",
      " [-1 -1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "\n",
      "batch 400\n",
      "  minibatch loss: 1.2325794696807861\n",
      "  sample 1:\n",
      "    input     > [6 7 2 9 0 0 0 0]\n",
      "    predicted > [[ 6  6  6]\n",
      " [ 6  2  2]\n",
      " [ 2  4  4]\n",
      " [ 4  6  9]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 2:\n",
      "    input     > [8 7 3 3 3 2 9 0]\n",
      "    predicted > [[8 8 8]\n",
      " [3 3 3]\n",
      " [3 3 3]\n",
      " [2 2 2]\n",
      " [3 5 5]\n",
      " [5 2 2]\n",
      " [9 5 9]\n",
      " [1 1 1]]\n",
      "  sample 3:\n",
      "    input     > [6 4 2 4 3 7 2 0]\n",
      "    predicted > [[4 4 4]\n",
      " [2 2 2]\n",
      " [7 2 2]\n",
      " [2 7 7]\n",
      " [4 4 4]\n",
      " [6 7 7]\n",
      " [4 6 2]\n",
      " [1 1 1]]\n",
      "\n",
      "batch 600\n",
      "  minibatch loss: 0.9292899370193481\n",
      "  sample 1:\n",
      "    input     > [4 9 5 9 9 2 0 0]\n",
      "    predicted > [[ 9  9  9]\n",
      " [ 4  4  4]\n",
      " [ 5  5  4]\n",
      " [ 9  9  9]\n",
      " [ 2  2  7]\n",
      " [ 4  5  2]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]]\n",
      "  sample 2:\n",
      "    input     > [8 2 6 4 7 0 0 0]\n",
      "    predicted > [[ 7  7  4]\n",
      " [ 2  2  2]\n",
      " [ 4  4  7]\n",
      " [ 6  6  6]\n",
      " [ 4  5  8]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 3:\n",
      "    input     > [7 9 7 9 6 0 0 0]\n",
      "    predicted > [[ 9  9  9]\n",
      " [ 7  7  7]\n",
      " [ 7  7  7]\n",
      " [ 6  9  9]\n",
      " [ 9  7  6]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "\n",
      "batch 800\n",
      "  minibatch loss: 0.7363898754119873\n",
      "  sample 1:\n",
      "    input     > [9 2 6 0 0 0 0 0]\n",
      "    predicted > [[ 9  2  9]\n",
      " [ 2  9  6]\n",
      " [ 6  6  2]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 2:\n",
      "    input     > [6 8 7 9 6 3 2 0]\n",
      "    predicted > [[ 6  6  6]\n",
      " [ 5  8  5]\n",
      " [ 3  6  6]\n",
      " [ 6  5  3]\n",
      " [ 9  6  3]\n",
      " [ 7  3  9]\n",
      " [ 3  2  7]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]]\n",
      "  sample 3:\n",
      "    input     > [9 2 8 4 9 6 9 3]\n",
      "    predicted > [[9 9 9]\n",
      " [3 2 3]\n",
      " [9 9 9]\n",
      " [2 8 4]\n",
      " [4 4 2]\n",
      " [9 6 9]\n",
      " [7 9 7]\n",
      " [8 7 8]\n",
      " [1 1 1]]\n",
      "\n",
      "batch 1000\n",
      "  minibatch loss: 0.7347214221954346\n",
      "  sample 1:\n",
      "    input     > [3 3 8 0 0 0 0 0]\n",
      "    predicted > [[ 3  3  3]\n",
      " [ 3  3  8]\n",
      " [ 8  3  6]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 2:\n",
      "    input     > [3 8 4 9 6 3 5 4]\n",
      "    predicted > [[ 3  3  3]\n",
      " [ 3  3  3]\n",
      " [ 4  4  4]\n",
      " [ 5  5  5]\n",
      " [ 9  6  6]\n",
      " [ 5  9  9]\n",
      " [ 6  4  4]\n",
      " [ 1  1  8]\n",
      " [-1 -1  1]]\n",
      "  sample 3:\n",
      "    input     > [3 4 7 0 0 0 0 0]\n",
      "    predicted > [[ 3  4  7]\n",
      " [ 4  3  4]\n",
      " [ 7  7  3]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "\n",
      "batch 1200\n",
      "  minibatch loss: 0.43508097529411316\n",
      "  sample 1:\n",
      "    input     > [5 5 5 4 4 4 4 0]\n",
      "    predicted > [[ 5  5  5]\n",
      " [ 5  5  4]\n",
      " [ 4  4  5]\n",
      " [ 4  5  5]\n",
      " [ 5  4  4]\n",
      " [ 5  4  5]\n",
      " [ 1  5  1]\n",
      " [-1  1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 2:\n",
      "    input     > [2 7 3 0 0 0 0 0]\n",
      "    predicted > [[ 2  7  2]\n",
      " [ 7  2  7]\n",
      " [ 3  3  6]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 3:\n",
      "    input     > [2 2 8 0 0 0 0 0]\n",
      "    predicted > [[ 2  2  2]\n",
      " [ 2  2  8]\n",
      " [ 8  5  2]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "\n",
      "batch 1400\n",
      "  minibatch loss: 0.41912826895713806\n",
      "  sample 1:\n",
      "    input     > [7 8 5 3 2 0 0 0]\n",
      "    predicted > [[ 5  7  8]\n",
      " [ 7  8  7]\n",
      " [ 8  5  5]\n",
      " [ 3  3  3]\n",
      " [ 2  2  2]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 2:\n",
      "    input     > [8 7 6 9 2 0 0 0]\n",
      "    predicted > [[ 8  7  8]\n",
      " [ 7  8  6]\n",
      " [ 6  9  7]\n",
      " [ 9  6  2]\n",
      " [ 2  2  9]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 3:\n",
      "    input     > [7 8 3 8 5 8 7 6]\n",
      "    predicted > [[8 8 8]\n",
      " [7 8 7]\n",
      " [8 7 8]\n",
      " [3 3 3]\n",
      " [7 7 7]\n",
      " [8 8 5]\n",
      " [6 6 3]\n",
      " [5 5 5]\n",
      " [1 1 1]]\n",
      "\n",
      "batch 1600\n",
      "  minibatch loss: 0.3989475965499878\n",
      "  sample 1:\n",
      "    input     > [3 7 4 4 7 2 0 0]\n",
      "    predicted > [[ 3  7  7]\n",
      " [ 7  3  4]\n",
      " [ 4  4  3]\n",
      " [ 4  4  3]\n",
      " [ 7  3  4]\n",
      " [ 2  2  2]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 2:\n",
      "    input     > [5 7 6 4 2 4 9 5]\n",
      "    predicted > [[5 5 5]\n",
      " [7 7 7]\n",
      " [6 6 6]\n",
      " [4 4 4]\n",
      " [2 2 2]\n",
      " [5 4 9]\n",
      " [9 9 5]\n",
      " [4 5 4]\n",
      " [1 1 1]]\n",
      "  sample 3:\n",
      "    input     > [7 6 4 0 0 0 0 0]\n",
      "    predicted > [[ 7  6  7]\n",
      " [ 6  7  4]\n",
      " [ 4  4  6]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "\n",
      "batch 1800\n",
      "  minibatch loss: 0.31475651264190674\n",
      "  sample 1:\n",
      "    input     > [8 5 9 9 9 4 0 0]\n",
      "    predicted > [[ 8  8  8]\n",
      " [ 5  9  9]\n",
      " [ 9  5  5]\n",
      " [ 9  4  9]\n",
      " [ 4  9  4]\n",
      " [ 9  9  9]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 2:\n",
      "    input     > [9 6 6 0 0 0 0 0]\n",
      "    predicted > [[ 9  6  6]\n",
      " [ 6  9  9]\n",
      " [ 6  6  9]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 3:\n",
      "    input     > [7 3 6 3 3 7 0 0]\n",
      "    predicted > [[ 3  7  7]\n",
      " [ 7  3  3]\n",
      " [ 6  6  3]\n",
      " [ 3  3  6]\n",
      " [ 7  3  7]\n",
      " [ 3  7  3]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "\n",
      "batch 2000\n",
      "  minibatch loss: 0.41449815034866333\n",
      "  sample 1:\n",
      "    input     > [6 6 8 0 0 0 0 0]\n",
      "    predicted > [[ 6  6  6]\n",
      " [ 6  9  3]\n",
      " [ 8  7  9]\n",
      " [ 1  1  7]\n",
      " [-1 -1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 2:\n",
      "    input     > [8 8 5 6 2 9 2 3]\n",
      "    predicted > [[8 8 8]\n",
      " [8 8 8]\n",
      " [5 5 6]\n",
      " [6 6 5]\n",
      " [2 2 2]\n",
      " [9 9 9]\n",
      " [2 3 2]\n",
      " [3 2 3]\n",
      " [1 1 1]]\n",
      "  sample 3:\n",
      "    input     > [5 2 3 2 4 7 7 6]\n",
      "    predicted > [[2 5 2]\n",
      " [5 2 5]\n",
      " [4 2 4]\n",
      " [3 3 3]\n",
      " [2 5 2]\n",
      " [7 7 7]\n",
      " [7 4 6]\n",
      " [6 6 5]\n",
      " [1 1 1]]\n",
      "\n",
      "batch 2200\n",
      "  minibatch loss: 0.2028750777244568\n",
      "  sample 1:\n",
      "    input     > [2 5 6 8 6 7 7]\n",
      "    predicted > [[2 2 2]\n",
      " [5 9 6]\n",
      " [6 7 5]\n",
      " [7 5 8]\n",
      " [9 7 7]\n",
      " [7 6 6]\n",
      " [8 5 7]\n",
      " [1 1 1]]\n",
      "  sample 2:\n",
      "    input     > [9 3 3 0 0 0 0]\n",
      "    predicted > [[ 9  9  3]\n",
      " [ 3  3  9]\n",
      " [ 3  6  9]\n",
      " [ 1  1  3]\n",
      " [-1 -1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 3:\n",
      "    input     > [7 9 9 0 0 0 0]\n",
      "    predicted > [[ 7  8  6]\n",
      " [ 9  6  8]\n",
      " [ 9  9  9]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "\n",
      "batch 2400\n",
      "  minibatch loss: 0.17885658144950867\n",
      "  sample 1:\n",
      "    input     > [3 6 8 3 2 0 0 0]\n",
      "    predicted > [[ 3  6  3]\n",
      " [ 6  3  6]\n",
      " [ 8  8  8]\n",
      " [ 3  3  3]\n",
      " [ 2  3  3]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 2:\n",
      "    input     > [4 5 4 6 5 5 0 0]\n",
      "    predicted > [[ 4  4  4]\n",
      " [ 5  5  5]\n",
      " [ 4  4  4]\n",
      " [ 5  6  5]\n",
      " [ 6  5  6]\n",
      " [ 5  5  4]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 3:\n",
      "    input     > [9 7 2 3 3 0 0 0]\n",
      "    predicted > [[ 9  9  9]\n",
      " [ 7  7  7]\n",
      " [ 3  2  3]\n",
      " [ 2  3  2]\n",
      " [ 3  3  6]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "\n",
      "batch 2600\n",
      "  minibatch loss: 0.20247018337249756\n",
      "  sample 1:\n",
      "    input     > [3 7 9 0 0 0 0 0]\n",
      "    predicted > [[ 3  7  3]\n",
      " [ 7  3  5]\n",
      " [ 9  9  2]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 2:\n",
      "    input     > [2 5 2 5 8 0 0 0]\n",
      "    predicted > [[ 2  2  2]\n",
      " [ 5  5  5]\n",
      " [ 2  2  2]\n",
      " [ 5  5  5]\n",
      " [ 8  5  3]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 3:\n",
      "    input     > [5 7 3 7 0 0 0 0]\n",
      "    predicted > [[ 5  5  7]\n",
      " [ 7  7  5]\n",
      " [ 3  7  5]\n",
      " [ 7  3  3]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "\n",
      "batch 2800\n",
      "  minibatch loss: 0.24160973727703094\n",
      "  sample 1:\n",
      "    input     > [8 2 2 8 4 0 0 0]\n",
      "    predicted > [[ 8  2  8]\n",
      " [ 2  8  2]\n",
      " [ 2  8  2]\n",
      " [ 8  2  4]\n",
      " [ 4  4  8]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 2:\n",
      "    input     > [3 3 8 7 0 0 0 0]\n",
      "    predicted > [[ 3  3  3]\n",
      " [ 3  3  8]\n",
      " [ 8  7  3]\n",
      " [ 7  8  7]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 3:\n",
      "    input     > [5 5 8 7 3 7 8 5]\n",
      "    predicted > [[5 5 5]\n",
      " [5 5 5]\n",
      " [8 8 8]\n",
      " [7 7 7]\n",
      " [3 3 7]\n",
      " [7 7 3]\n",
      " [8 5 8]\n",
      " [5 8 5]\n",
      " [1 1 1]]\n",
      "\n",
      "batch 3000\n",
      "  minibatch loss: 0.23292377591133118\n",
      "  sample 1:\n",
      "    input     > [4 4 2 7 0 0 0 0]\n",
      "    predicted > [[ 4  4  4]\n",
      " [ 4  4  2]\n",
      " [ 2  7  4]\n",
      " [ 7  2  7]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 2:\n",
      "    input     > [9 5 3 4 8 7 6 9]\n",
      "    predicted > [[9 9 9]\n",
      " [5 5 5]\n",
      " [3 3 8]\n",
      " [4 4 3]\n",
      " [8 8 4]\n",
      " [7 7 6]\n",
      " [9 6 7]\n",
      " [6 9 9]\n",
      " [1 1 1]]\n",
      "  sample 3:\n",
      "    input     > [5 5 2 4 2 0 0 0]\n",
      "    predicted > [[ 5  5  5]\n",
      " [ 5  5  5]\n",
      " [ 2  4  2]\n",
      " [ 4  2  2]\n",
      " [ 2  2  4]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "\n",
      "batch 3200\n",
      "  minibatch loss: 0.13823337852954865\n",
      "  sample 1:\n",
      "    input     > [3 3 7 8 2 3 7 2]\n",
      "    predicted > [[3 3 3]\n",
      " [3 3 3]\n",
      " [7 8 8]\n",
      " [8 7 7]\n",
      " [2 2 2]\n",
      " [3 3 3]\n",
      " [7 7 2]\n",
      " [2 2 7]\n",
      " [1 1 1]]\n",
      "  sample 2:\n",
      "    input     > [6 2 7 3 5 4 7 2]\n",
      "    predicted > [[6 6 6]\n",
      " [2 2 2]\n",
      " [7 7 7]\n",
      " [3 3 5]\n",
      " [4 5 3]\n",
      " [5 4 2]\n",
      " [7 2 4]\n",
      " [2 7 7]\n",
      " [1 1 1]]\n",
      "  sample 3:\n",
      "    input     > [2 2 7 7 2 0 0 0]\n",
      "    predicted > [[ 2  2  2]\n",
      " [ 2  7  2]\n",
      " [ 7  2  7]\n",
      " [ 7  2  2]\n",
      " [ 2  7  7]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "\n",
      "batch 3400\n",
      "  minibatch loss: 0.118137888610363\n",
      "  sample 1:\n",
      "    input     > [5 5 7 7 6 0 0 0]\n",
      "    predicted > [[ 5  5  5]\n",
      " [ 5  7  7]\n",
      " [ 7  5  5]\n",
      " [ 7  5  5]\n",
      " [ 6  7  6]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 2:\n",
      "    input     > [8 2 4 5 0 0 0 0]\n",
      "    predicted > [[ 8  4  8]\n",
      " [ 2  8  2]\n",
      " [ 4  2  4]\n",
      " [ 5  5  4]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 3:\n",
      "    input     > [3 5 2 4 0 0 0 0]\n",
      "    predicted > [[ 3  3  3]\n",
      " [ 5  5  5]\n",
      " [ 2  2  2]\n",
      " [ 4  5  8]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "\n",
      "batch 3600\n",
      "  minibatch loss: 0.18091285228729248\n",
      "  sample 1:\n",
      "    input     > [9 2 3 2 7 6 6 3]\n",
      "    predicted > [[9 9 9]\n",
      " [2 2 2]\n",
      " [3 2 3]\n",
      " [2 3 2]\n",
      " [6 7 7]\n",
      " [7 6 6]\n",
      " [6 6 6]\n",
      " [3 3 3]\n",
      " [1 1 1]]\n",
      "  sample 2:\n",
      "    input     > [9 7 6 8 5 3 0 0]\n",
      "    predicted > [[ 9  9  9]\n",
      " [ 7  7  7]\n",
      " [ 6  6  9]\n",
      " [ 8  8  7]\n",
      " [ 5  5  3]\n",
      " [ 3  7  5]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 3:\n",
      "    input     > [3 4 4 9 8 8 8 5]\n",
      "    predicted > [[3 3 3]\n",
      " [4 4 4]\n",
      " [4 4 9]\n",
      " [9 8 8]\n",
      " [8 9 4]\n",
      " [8 5 8]\n",
      " [5 8 5]\n",
      " [8 8 5]\n",
      " [1 1 1]]\n",
      "\n",
      "batch 3800\n",
      "  minibatch loss: 0.1578817516565323\n",
      "  sample 1:\n",
      "    input     > [4 9 4 5 4 3 9 0]\n",
      "    predicted > [[ 4  4  4]\n",
      " [ 4  9  4]\n",
      " [ 9  4  9]\n",
      " [ 5  5  5]\n",
      " [ 9  4  9]\n",
      " [ 8  3  3]\n",
      " [ 3  9  4]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]]\n",
      "  sample 2:\n",
      "    input     > [8 3 3 4 0 0 0 0]\n",
      "    predicted > [[ 8  3  3]\n",
      " [ 3  8  8]\n",
      " [ 3  8  8]\n",
      " [ 4  3  4]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 3:\n",
      "    input     > [5 4 7 9 5 0 0 0]\n",
      "    predicted > [[ 5  5  5]\n",
      " [ 4  4  7]\n",
      " [ 7  5  4]\n",
      " [ 9  7  9]\n",
      " [ 5  9  4]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "\n",
      "batch 4000\n",
      "  minibatch loss: 0.21402882039546967\n",
      "  sample 1:\n",
      "    input     > [2 4 9 4 4 3 2 0]\n",
      "    predicted > [[ 4  2  4]\n",
      " [ 2  4  2]\n",
      " [ 9  9  9]\n",
      " [ 4  4  4]\n",
      " [ 2  4  2]\n",
      " [ 8  3  8]\n",
      " [ 4  2  1]\n",
      " [ 1  1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 2:\n",
      "    input     > [7 8 5 0 0 0 0 0]\n",
      "    predicted > [[ 7  7  7]\n",
      " [ 8  8  8]\n",
      " [ 5  8  9]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 3:\n",
      "    input     > [6 6 2 0 0 0 0 0]\n",
      "    predicted > [[ 6  6  6]\n",
      " [ 6  2  6]\n",
      " [ 2  6  4]\n",
      " [ 1  1  2]\n",
      " [-1 -1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "\n",
      "batch 4200\n",
      "  minibatch loss: 0.07165724784135818\n",
      "  sample 1:\n",
      "    input     > [5 8 2 6 0 0 0 0]\n",
      "    predicted > [[ 5  8  8]\n",
      " [ 8  5  5]\n",
      " [ 2  6  2]\n",
      " [ 6  2  6]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 2:\n",
      "    input     > [4 3 9 8 7 3 9 2]\n",
      "    predicted > [[4 4 4]\n",
      " [3 3 3]\n",
      " [9 9 9]\n",
      " [8 7 8]\n",
      " [7 8 7]\n",
      " [3 3 3]\n",
      " [9 9 2]\n",
      " [2 3 9]\n",
      " [1 1 1]]\n",
      "  sample 3:\n",
      "    input     > [4 2 3 8 2 0 0 0]\n",
      "    predicted > [[ 4  2  4]\n",
      " [ 2  4  3]\n",
      " [ 3  8  2]\n",
      " [ 8  3  2]\n",
      " [ 2  2  8]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "\n",
      "batch 4400\n",
      "  minibatch loss: 0.08584733307361603\n",
      "  sample 1:\n",
      "    input     > [5 6 4 5 2 5 5]\n",
      "    predicted > [[5 5 5]\n",
      " [6 6 6]\n",
      " [4 4 4]\n",
      " [5 5 5]\n",
      " [2 6 2]\n",
      " [5 2 5]\n",
      " [5 5 6]\n",
      " [1 1 1]]\n",
      "  sample 2:\n",
      "    input     > [5 5 8 7 9 3 0]\n",
      "    predicted > [[ 5  5  5]\n",
      " [ 5  5  5]\n",
      " [ 8  8  8]\n",
      " [ 9  7  6]\n",
      " [ 7  9  8]\n",
      " [ 3  3  7]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]]\n",
      "  sample 3:\n",
      "    input     > [9 5 9 2 5 7 3]\n",
      "    predicted > [[9 9 9]\n",
      " [5 5 5]\n",
      " [9 9 9]\n",
      " [2 2 2]\n",
      " [5 7 5]\n",
      " [7 5 7]\n",
      " [3 3 7]\n",
      " [1 1 1]]\n",
      "\n",
      "batch 4600\n",
      "  minibatch loss: 0.08049434423446655\n",
      "  sample 1:\n",
      "    input     > [3 9 4 0 0 0 0 0]\n",
      "    predicted > [[ 3  3  9]\n",
      " [ 9  4  3]\n",
      " [ 4  9  4]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 2:\n",
      "    input     > [7 8 9 4 3 5 2 3]\n",
      "    predicted > [[7 7 8]\n",
      " [8 8 7]\n",
      " [9 9 9]\n",
      " [4 4 4]\n",
      " [3 3 3]\n",
      " [5 7 5]\n",
      " [2 9 2]\n",
      " [3 3 3]\n",
      " [1 1 1]]\n",
      "  sample 3:\n",
      "    input     > [9 3 6 4 4 6 5 9]\n",
      "    predicted > [[9 9 9]\n",
      " [3 3 3]\n",
      " [6 4 6]\n",
      " [4 6 4]\n",
      " [4 6 4]\n",
      " [6 5 5]\n",
      " [5 4 6]\n",
      " [9 9 6]\n",
      " [1 1 1]]\n",
      "\n",
      "batch 4800\n",
      "  minibatch loss: 0.037724826484918594\n",
      "  sample 1:\n",
      "    input     > [5 6 8 2 5 0 0]\n",
      "    predicted > [[ 5  6  6]\n",
      " [ 6  5  5]\n",
      " [ 8  8  8]\n",
      " [ 2  2  5]\n",
      " [ 5  5  2]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 2:\n",
      "    input     > [7 9 5 0 0 0 0]\n",
      "    predicted > [[ 7  5  5]\n",
      " [ 9  7  2]\n",
      " [ 5  9  7]\n",
      " [ 1  1  9]\n",
      " [-1 -1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 3:\n",
      "    input     > [5 3 3 6 0 0 0]\n",
      "    predicted > [[ 5  3  3]\n",
      " [ 3  5  5]\n",
      " [ 3  5  3]\n",
      " [ 6  3  5]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "\n",
      "batch 5000\n",
      "  minibatch loss: 0.12354864180088043\n",
      "  sample 1:\n",
      "    input     > [4 6 9 6 5 7 8 9]\n",
      "    predicted > [[4 4 4]\n",
      " [6 6 6]\n",
      " [9 9 6]\n",
      " [6 6 9]\n",
      " [5 5 5]\n",
      " [7 7 8]\n",
      " [9 8 6]\n",
      " [8 9 7]\n",
      " [1 1 1]]\n",
      "  sample 2:\n",
      "    input     > [6 5 9 9 8 0 0 0]\n",
      "    predicted > [[ 6  6  6]\n",
      " [ 5  5  9]\n",
      " [ 9  9  5]\n",
      " [ 9  8  8]\n",
      " [ 8  9  9]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 3:\n",
      "    input     > [6 7 2 8 9 7 0 0]\n",
      "    predicted > [[ 6  6  6]\n",
      " [ 7  7  7]\n",
      " [ 2  2  2]\n",
      " [ 8  8  8]\n",
      " [ 9  6  7]\n",
      " [ 7  8  9]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "\n",
      "batch 5200\n",
      "  minibatch loss: 0.05009409785270691\n",
      "  sample 1:\n",
      "    input     > [6 3 8 7 0 0 0]\n",
      "    predicted > [[ 6  6  6]\n",
      " [ 3  8  8]\n",
      " [ 8  3  3]\n",
      " [ 7  7  6]\n",
      " [ 1  1  7]\n",
      " [-1 -1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 2:\n",
      "    input     > [9 2 5 9 0 0 0]\n",
      "    predicted > [[ 9  2  9]\n",
      " [ 2  9  5]\n",
      " [ 5  5  2]\n",
      " [ 9  9  9]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 3:\n",
      "    input     > [3 5 2 7 7 6 0]\n",
      "    predicted > [[ 3  5  3]\n",
      " [ 5  3  5]\n",
      " [ 2  2  7]\n",
      " [ 7  7  2]\n",
      " [ 7  6  2]\n",
      " [ 6  7  5]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]]\n",
      "\n",
      "batch 5400\n",
      "  minibatch loss: 0.09247519075870514\n",
      "  sample 1:\n",
      "    input     > [8 6 5 3 8 7 4 2]\n",
      "    predicted > [[8 8 8]\n",
      " [6 6 6]\n",
      " [5 5 5]\n",
      " [3 8 8]\n",
      " [8 3 3]\n",
      " [7 7 2]\n",
      " [4 2 7]\n",
      " [2 4 4]\n",
      " [1 1 1]]\n",
      "  sample 2:\n",
      "    input     > [8 6 3 9 4 7 5 0]\n",
      "    predicted > [[ 8  3  3]\n",
      " [ 6  8  8]\n",
      " [ 3  9  9]\n",
      " [ 9  6  6]\n",
      " [ 4  4  7]\n",
      " [ 7  7  4]\n",
      " [ 5  7  4]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]]\n",
      "  sample 3:\n",
      "    input     > [5 3 8 8 6 6 3 0]\n",
      "    predicted > [[ 5  5  5]\n",
      " [ 8  3  8]\n",
      " [ 3  8  3]\n",
      " [ 6  8  6]\n",
      " [ 8  6  3]\n",
      " [ 3  6  8]\n",
      " [ 6  3  9]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]]\n",
      "\n",
      "batch 5600\n",
      "  minibatch loss: 0.05249354988336563\n",
      "  sample 1:\n",
      "    input     > [5 9 5 0 0 0 0 0]\n",
      "    predicted > [[ 5  5  5]\n",
      " [ 9  9  5]\n",
      " [ 5  8  9]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 2:\n",
      "    input     > [9 6 6 7 3 6 5 6]\n",
      "    predicted > [[ 9  9  9]\n",
      " [ 6  6  6]\n",
      " [ 6  6  6]\n",
      " [ 7  3  7]\n",
      " [ 3  7  3]\n",
      " [ 6  6  6]\n",
      " [ 5  5  5]\n",
      " [ 6  6  1]\n",
      " [ 1  1 -1]]\n",
      "  sample 3:\n",
      "    input     > [6 3 9 5 9 9 3 0]\n",
      "    predicted > [[ 6  3  3]\n",
      " [ 3  6  6]\n",
      " [ 9  9  9]\n",
      " [ 5  5  5]\n",
      " [ 9  9  9]\n",
      " [ 9  9  9]\n",
      " [ 3  3  8]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]]\n",
      "\n",
      "batch 5800\n",
      "  minibatch loss: 0.08289551734924316\n",
      "  sample 1:\n",
      "    input     > [8 9 3 5 2 0 0 0]\n",
      "    predicted > [[ 8  8  9]\n",
      " [ 9  9  8]\n",
      " [ 3  5  3]\n",
      " [ 5  3  5]\n",
      " [ 2  2  2]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 2:\n",
      "    input     > [7 4 8 3 2 3 4 9]\n",
      "    predicted > [[ 7  7  7]\n",
      " [ 4  4  4]\n",
      " [ 8  8  3]\n",
      " [ 3  3  8]\n",
      " [ 2  2  2]\n",
      " [ 3  3  4]\n",
      " [ 4  9  3]\n",
      " [ 9  4  9]\n",
      " [ 1  1  6]\n",
      " [-1 -1  1]]\n",
      "  sample 3:\n",
      "    input     > [2 3 6 6 0 0 0 0]\n",
      "    predicted > [[ 2  2  3]\n",
      " [ 3  3  2]\n",
      " [ 6  6  6]\n",
      " [ 6  9  6]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "\n",
      "batch 6000\n",
      "  minibatch loss: 0.03706203028559685\n",
      "  sample 1:\n",
      "    input     > [3 5 3 9 0 0 0 0]\n",
      "    predicted > [[ 3  3  5]\n",
      " [ 5  3  3]\n",
      " [ 3  5  3]\n",
      " [ 9  9  9]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "  sample 2:\n",
      "    input     > [8 2 9 6 2 8 2 3]\n",
      "    predicted > [[8 8 8]\n",
      " [2 2 9]\n",
      " [9 9 2]\n",
      " [6 6 2]\n",
      " [2 2 6]\n",
      " [8 8 8]\n",
      " [3 2 3]\n",
      " [2 3 9]\n",
      " [1 1 1]]\n",
      "  sample 3:\n",
      "    input     > [6 4 8 4 9 7 4 0]\n",
      "    predicted > [[ 6  6  6]\n",
      " [ 4  4  4]\n",
      " [ 8  8  4]\n",
      " [ 4  9  8]\n",
      " [ 9  4  8]\n",
      " [ 7  7  9]\n",
      " [ 4  4  6]\n",
      " [ 1  1  1]\n",
      " [-1 -1 -1]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss_track = []\n",
    "max_batches = 6001\n",
    "batches_in_epoch = 200\n",
    "\n",
    "try:\n",
    "    # 一个epoch的learning\n",
    "    for batch in range(max_batches):\n",
    "        fd = next_feed()\n",
    "        _, l = sess.run([train_op, loss], fd)\n",
    "        loss_track.append(l)\n",
    "        \n",
    "        if batch == 0 or batch % batches_in_epoch == 0:\n",
    "            print('batch {}'.format(batch))\n",
    "            print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
    "            predict_ = sess.run(beam_decoder_outputs.predicted_ids, fd)\n",
    "            #print(predict_)\n",
    "            for i, (inp, pred) in enumerate(zip(fd[encoder_inputs], predict_)):\n",
    "                print('  sample {}:'.format(i + 1))\n",
    "                print('    input     > {}'.format(inp))\n",
    "                print('    predicted > {}'.format(pred))\n",
    "                if i >= 2:\n",
    "                    break\n",
    "            print()\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print('training interrupted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.0375 after 60010 examples (batch_size=10)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_track)\n",
    "print('loss {:.4f} after {} examples (batch_size={})'.format(loss_track[-1], \n",
    "                                                             len(loss_track)*batch_size, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
